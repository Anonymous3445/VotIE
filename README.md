# VotIE: Voting Information Extraction from Municipal Meeting Minutes


[![License: CC-BY-ND 4.0](https://img.shields.io/badge/License-CC--BY--ND%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nd/4.0/)
[![Python 3.10](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

Official repository for the submission of the paper **"VotIE: Voting Information Extraction from Municipal Meeting Minutes"**, for ECIR 2026.

This repository provides a comprehensive framework for extracting structured voting information from Portuguese municipal meeting minutes using span extraction using sequence labeling, along with voting events construction heuristics.
> **ðŸŽ¯ Try VotIE Now**: Test the model interactively at [huggingface.co/spaces/Anonymous3445/VotIE-demo](https://huggingface.co/spaces/Anonymous3445/VotIE-demo)

---

## Table of Contents

- [Overview](#overview)
- [Key Features](#key-features)
- [Project Status](#project-status)
- [Technology Stack](#technology-stack)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage](#usage)
  - [Training Models](#training-models)
  - [Making Predictions](#making-predictions)
  - [Running Evaluation](#running-evaluation)
- [Dataset](#dataset)
- [Models](#models)
- [Experimental Results](#experimental-results)
- [Repository Structure](#repository-structure)
- [Reproducibility](#reproducibility)
- [Citation](#citation)
- [License](#license)
- [Acknowledgments](#acknowledgments)

---

## Overview

VotIE is a specialized pipeline designed to extract structured voting information from Portuguese municipal meeting minutes. The system operates at two levels:

1. **Entity-Level Extraction** â€“ Identifies and classifies voting-related spans using BIO tagging
2. **Event-Level Construction** â€“ Reconstructs complete voting events from extracted entities

The framework supports 8 entity types:
- **VOTER-FAVOR** â€“ Participants who voted in favor
- **VOTER-AGAINST** â€“ Participants who voted against
- **VOTER-ABSTENTION** â€“ Participants who abstained
- **VOTER-ABSENT** â€“ Participants who were absent
- **VOTING** â€“ Voting action expressions (e.g., "deliberated", "approved")
- **SUBJECT** â€“ The subject matter being voted on
- **COUNTING-UNANIMITY** â€“ Unanimous vote indicators
- **COUNTING-MAJORITY** â€“ Majority vote indicators

---

## Key Features

- **Dual-Level Evaluation**: Entity-level (token-level F1) and Event-level (complete event accuracy) metrics
- **Multiple Architectures**: Traditional baselines (CRF, BiLSTM+FastText); Transformer models (BERTimbau, DeBERTa, XLM-RoBERTa); Gemini 2.5 flash for the event extraction, using gemini API and few-shot approach. 
- **Portuguese-Optimized**: Specialized for Portuguese administrative text with support for BERTimbau
- **Windowing Support**: Handles long documents that exceed transformer context limits
- **Production-Ready**: Modular, well-documented codebase with comprehensive evaluation
- **Reproducible**: Fixed seeds, documented hyperparameters, and detailed training logs

---

## Project Status

The VotIE framework is **fully implemented and validated** for research use. The codebase is actively maintained to ensure reproducibility of published results. Complete dataset not yet available. 

---

## Technology Stack

### Core Frameworks
- **PyTorch** (2.0+) â€“ Deep learning backend
- **Transformers (Hugging Face)** (4.20+) â€“ Pre-trained language models
- **seqeval** (1.2.2+) â€“ Sequence labeling evaluation

### NLP Libraries
- **sklearn-crfsuite** (0.3.6+) â€“ Traditional CRF baseline
- **pytorch-crf** (0.7.2+) â€“ CRF layer for transformers
- **FastText** â€“ Word embeddings for BiLSTM baseline
- **spaCy** (3.4.0+) â€“ Text preprocessing

### Utilities
- **PyYAML** (6.0+) â€“ Configuration management
- **pandas** â€“ Data analysis and result formatting
- **tqdm** â€“ Progress tracking

### Hardware
- All experiments were conducted on a Nvidia L40 GPU
- Recommended: GPU with 8GB+ VRAM for transformer models

---

## Installation

### Prerequisites
- Python 3.10
- CUDA 11.7+ (for GPU support)
- Git

### Step 1: Clone the Repository
```bash
git clone https://github.com/Anonymous3445/votie.git
cd votie
```

### Step 2: Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

---

## Quick Start

**Option 1: Try the Interactive Demo (Recommended)**

ðŸŽ¯ **[Live Demo on Hugging Face Spaces](https://huggingface.co/spaces/Anonymous3445/VotIE-demo)**

Try VotIE directly in your browser without any installation! The interactive demo allows you to:
- Test the model on sample Portuguese municipal texts
- Upload your own meeting minutes
- Visualize entity extraction results in real-time
- Explore voting event reconstruction

**Option 2: Evaluate on Sample Data (Best for Reviewers)**

Run the pre-trained model on the 30 sample examples included in this repository:

```bash
# Evaluate all 30 examples
python scripts/evaluate_sample_data.py

# Quick test with just 5 examples (faster)
python scripts/evaluate_sample_data.py --limit 5
```

This will:
- Load the HuggingFace model automatically
- Run predictions on sample data from `data/data_examples.json`
- Generate comprehensive evaluation metrics
- Save results to `results/` directory

**Option 3: Use Pre-trained Model Locally**

For a quick test of the model on your own text, see `scripts/quick_start.py`:

```python
from transformers import AutoTokenizer, AutoModel

# Load model and tokenizer
model_name = "Anonymous3445/DeBERTa-CRF-VotIE"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name, trust_remote_code=True)

# Example text
text = "A CÃ¢mara deliberou aprovar a proposta por unanimidade."

# Tokenize
inputs = tokenizer(text, return_tensors="pt")

# Get predictions
predictions = model.decode(**inputs, tokenizer=tokenizer, text=text)

# Print results
for pred in predictions:
    print(f"{pred['word']:<30} {pred['label']}")
```

**Note:** The HuggingFace model is designed for inference on raw text and uses its own tokenization. For evaluation on the pre-tokenized test dataset, use the training pipeline (Options 4 or 5 below).

**Option 4: Train from Scratch - Full Pipeline** âš ï¸ (requires full dataset - not yet available)
```bash
# Train, predict, and evaluate in one command
python scripts/run_pipeline.py --config configs/deberta_crf.yaml --name my_experiment
```

**Option 5: Train from Scratch - Step by Step** âš ï¸ (requires full dataset - not yet available)
```bash
# 1. Train model
python scripts/train.py --config configs/deberta_crf.yaml --experiment-name my_experiment

# 2. Make predictions
python scripts/predict.py \
  models/deberta_crf/my_experiment/best_model \
  data/votie_bio/test.jsonl \
  predictions/my_predictions.jsonl

# 3. Evaluate
python scripts/evaluate.py \
  predictions/my_predictions.jsonl \
  evaluation/my_results.json
```

---

### Training Models

> **âš ï¸ Note**: Training requires the full VotIE dataset (not yet available in this repository). For now, use the [pre-trained model](https://huggingface.co/Anonymous3445/DeBERTa-CRF-VotIE) or test with the [ðŸŽ¯ Interactive Demo](https://huggingface.co/spaces/Anonymous3445/VotIE-demo).

The unified training script `scripts/train.py` supports all model architectures. Model type is automatically determined from the configuration file.

#### Basic Training
```bash
python scripts/train.py --config configs/MODEL_NAME.yaml
```

#### Available Model Configurations

**Traditional Baselines:**
- `configs/crf.yaml` â€“ Conditional Random Fields
- `configs/bilstm_fasttext.yaml` â€“ BiLSTM + FastText embeddings

**Transformer Baselines:**
- `configs/bert_linear.yaml` â€“ BERTimbau + Linear layer
- `configs/bert_crf.yaml` â€“ BERTimbau + CRF layer
- `configs/deberta_linear.yaml` â€“ DeBERTa + Linear layer
- `configs/deberta_crf.yaml` â€“ DeBERTa + CRF layer
- `configs/xlmr_linear.yaml` â€“ XLM-RoBERTa + Linear layer
- `configs/xlmr_crf.yaml` â€“ XLM-RoBERTa + CRF layer

#### Training Examples
```bash
# Train CRF baseline
python scripts/train.py --config configs/crf.yaml

# Train BERTimbau+CRF with custom experiment name
python scripts/train.py --config configs/bert_crf.yaml --experiment-name exp_001

```

#### Training Output
Trained models are saved to `models/MODEL_NAME/EXPERIMENT_NAME/`:
```
models/deberta_crf/run_20251017_120000/
â”œâ”€â”€ best_model/              # Best checkpoint by validation F1
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ training_results.json    # Training metrics and hyperparameters
â””â”€â”€ logs/                    # Training logs
```

**Note**:
- Training only performs model training and saves to `models/MODEL_NAME/EXPERIMENT_NAME/`
- Model checkpoints are NOT tracked in Git (too large)
- For the complete pipeline (train â†’ predict â†’ evaluate), use `scripts/run_pipeline.py` (see [Full Pipeline](#full-pipeline) section)
- To use the published best model, download from [Hugging Face](https://huggingface.co/Anonymous3445/DeBERTa-CRF-VotIE)

### Making Predictions

Generate predictions on new data:

```bash
python scripts/predict.py MODEL_PATH INPUT_JSONL OUTPUT_JSONL [DEVICE]
```

**Examples:**
```bash
# Predict on test set (CPU)
python scripts/predict.py \
  models/deberta_crf/run_20251017_120000/best_model \
  data/votie_bio/test.jsonl \
  predictions/test_predictions.jsonl \
  cpu

# Predict on test set (GPU)
python scripts/predict.py \
  models/deberta_crf/run_20251017_120000/best_model \
  data/votie_bio/test.jsonl \
  predictions/test_predictions.jsonl \
  cuda
```

**Input Format (JSONL):**
```json
{"id": "example_001", "tokens": ["O", "Executivo", "aprovou", "..."], "labels": ["O", "B-VOTER-FAVOR", "B-VOTING", "..."]}
```

**Output Format (JSONL):**
```json
{"id": "example_001", "tokens": ["O", "Executivo", "aprovou", "..."], "pred_labels": ["O", "B-VOTER-FAVOR", "B-VOTING", "..."], "gold_labels": ["O", "B-VOTER-FAVOR", "B-VOTING", "..."]}
```

### Running Evaluation

Evaluate predictions with comprehensive metrics:

```bash
python scripts/evaluate.py PREDICTIONS_JSONL [OUTPUT_JSON] [--no-events]
```

**Examples:**
```bash
# Evaluate with entity and event metrics
python scripts/evaluate.py predictions/test_predictions.jsonl

# Save detailed results to file
python scripts/evaluate.py \
  predictions/test_predictions.jsonl \
  evaluation/detailed_results.json

# Entity-level metrics only (faster)
python scripts/evaluate.py \
  predictions/test_predictions.jsonl \
  --no-events
```

**Metrics Computed:**

*Entity-Level (Span-based):*
- Precision, Recall, F1 (overall and per-entity-type)
- Uses `seqeval` library for strict BIO validation

*Event-Level (Voting Event):*
- Event Detection F1 â€“ Identifying documents with voting events
- Subject F1 â€“ Correct subject extraction
- Counting F1 â€“ Correct counting method (unanimity/majority)
- Participant-Vote F1 â€“ Correct participant-vote pairs
- Complete Event F1 â€“ All event components correct

### Full Pipeline

> **âš ï¸ Note**: The following pipelines require the full VotIE dataset (not yet available). Use the [ðŸŽ¯ Interactive Demo](https://huggingface.co/spaces/Anonymous3445/VotIE-demo) to test the model on sample data.

For convenience, use the pipeline script to run train â†’ predict â†’ evaluate in one command:

```bash
python scripts/run_pipeline.py --config configs/deberta_crf.yaml
```

This will:
1. Train the model
2. Generate predictions on the test set
3. Compute comprehensive evaluation metrics
4. Save all outputs to organized directories

### Running All Experiments

To reproduce all paper experiments (requires full dataset):

```bash
# Run all model configurations
bash scripts/run_all_experiments.sh
```

This will train all baselines (CRF, BiLSTM, BERT, DeBERTa, XLM-R) and generate complete results.

---

## Dataset

### VotIE Corpus

The VotIE corpus consists of voting segments extracted from Portuguese municipal meeting minutes.

> **âš ï¸ Important Note for Reviewers**:
> - **Full Dataset**: The complete dataset statistics are shown below, but the full dataset files are **not yet available** in this repository
> - **Sample Data**: This repository includes **30 annotated examples** from the test set (see [`data/data_examples.json`](data/data_examples.json) and [`data/data_statistics.json`](data/data_statistics.json))
> - **Interactive Testing**: To test the model on these examples and explore the full capabilities, please visit our **[ðŸŽ¯ Interactive Demo](https://huggingface.co/spaces/Anonymous3445/VotIE-demo)**

**Full Dataset Statistics** (for reference - full data not yet available in repo):

| **Attribute** | **Value** |
|---------------|-----------|
| **Language** | Portuguese |
| **Municipalities** | 6 (M01â€“M06) |
| **Total Examples** | 2,879 voting segments |
| **Total Tokens** | ~1,008,324 tokens |
| **Total Entities** | 9,951 annotated entities |
| **Train Examples** | 2,015 (70%) |
| **Dev Examples** | 431 (15%) |
| **Test Examples** | 433 (15%) |

**Sample Data Available in Repository**:

| **Attribute** | **Value** |
|---------------|-----------|
| **Examples Included** | 30 test set samples |
| **Municipalities Covered** | All 6 (M01â€“M06) |
| **Entity Types** | All 8 types represented |
| **File Locations** | [`data/data_examples.json`](data/data_examples.json), [`data/data_statistics.json`](data/data_statistics.json) |

### Entity Distribution (Full Dataset)

The following statistics are from the **full VotIE corpus** (not yet available in repository):

| **Entity Type** | **Train** | **Dev** | **Test** | **Total** |
|-----------------|-----------|---------|----------|-----------|
| VOTING | 1,866 | 385 | 397 | 2,648 |
| SUBJECT | 1,731 | 374 | 373 | 2,478 |
| VOTER-FAVOR | 1,150 | 245 | 255 | 1,650 |
| COUNTING-UNANIMITY | 1,092 | 251 | 227 | 1,570 |
| VOTER-ABSTENTION | 738 | 166 | 138 | 1,042 |
| COUNTING-MAJORITY | 202 | 34 | 52 | 288 |
| VOTER-AGAINST | 116 | 21 | 40 | 177 |
| VOTER-ABSENT | 62 | 14 | 22 | 98 |

For entity distribution in the **sample data** (30 examples), see [`data/data_statistics.json`](data/data_statistics.json).

### Data Format

The VotIE dataset uses two complementary formats for annotation:

**BIO-Tagged Format** (for entity-level extraction):
```json
{
  "id": "M01_cm_003_2023-02-01_seg020",
  "text": "O Executivo Municipal deliberou por unanimidade aprovar...",
  "tokens": ["O", "Executivo", "Municipal", "deliberou", "por", "unanimidade", "aprovar", "..."],
  "labels": ["O", "B-VOTER-FAVOR", "I-VOTER-FAVOR", "B-VOTING", "O", "B-COUNTING-UNANIMITY", "O", "..."],
  "municipality": "M01"
}
```

**Event-Level Format** (for voting event construction):
```json
{
  "id": "M01_cm_003_2023-02-01_seg020",
  "text": "...",
  "municipality": "M01",
  "has_voting_event": true,
  "event": {
    "subject": "atribuiÃ§Ã£o de trÃªs apoios",
    "participants": [{"text": "Executivo Municipal", "position": "Favor"}],
    "counting": [{"text": "por unanimidade", "type": "Unanimity"}],
    "voting_expressions": ["deliberou"],
    "outcome": "Approved"
  }
}
```

**Sample Data**: 30 annotated examples in both formats are available in [`data/data_examples.json`](data/data_examples.json).

**Full Dataset Directories** (referenced in code, not yet available):
- `data/votie_bio/` - BIO-tagged JSONL files (train.jsonl, dev.jsonl, test.jsonl)
- `data/votie_events/` - Event-level JSON files (train.json, dev.json, test.json)

---

## Models

### Model Architectures

All models follow a token classification architecture for BIO tagging:

**Traditional Baselines:**
1. **CRF** â€“ Conditional Random Fields with hand-crafted features
2. **BiLSTM+FastText** â€“ Bidirectional LSTM with pre-trained FastText embeddings

**Transformer Baselines:**

3. **BERTimbau-Large + Linear** â€“ Portuguese BERT with linear classification head
4. **BERTimbau-Large + CRF** â€“ Portuguese BERT with CRF layer
5. **DeBERTa-V3-Base + Linear** â€“ DeBERTa V3 with linear head
6. **DeBERTa-V3-Base + CRF** â€“ DeBERTa V3 with CRF layer
7. **XLM-RoBERTa-Large + Linear** â€“ Multilingual XLM-R with linear head
8. **XLM-RoBERTa-Large + CRF** â€“ Multilingual XLM-R with CRF layer

### Pre-trained Models

The best-performing model from the paper is available on Hugging Face:

- **DeBERTa-V3-Base + CRF** (Best Model): [`Anonymous3445/DeBERTa-CRF-VotIE`](https://huggingface.co/Anonymous3445/DeBERTa-CRF-VotIE)
- **ðŸŽ¯ Interactive Demo**: [VotIE-demo on Hugging Face Spaces](https://huggingface.co/spaces/Anonymous3445/VotIE-demo) â€“ Try the model directly in your browser

**Usage:**
```python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("Anonymous3445/DeBERTa-CRF-VotIE")
model = AutoModel.from_pretrained("Anonymous3445/DeBERTa-CRF-VotIE")
```

To train other models from scratch, use the configurations in `configs/` directory.

### Configuration

All model hyperparameters are specified in YAML configuration files:

```yaml
model:
  name: "deberta_crf"
  model_name: "microsoft/deberta-v3-base"
  architecture: "crf"
  dropout: 0.1

data:
  data_dir: "data/votie_bio"
  train_file: "train.jsonl"
  dev_file: "dev.jsonl"
  test_file: "test.jsonl"

training:
  batch_size: 16
  epochs: 10
  learning_rate: 5e-5
  warmup_proportion: 0.1
  weight_decay: 0.01
  patience: 3
  device: 0  # GPU device ID
  seed: 42

windowing:
  enable_windowing: true
  overlap_tokens: 50
```

---

## Experimental Results

### Main Results (Test Set)

All experimental results are available in `paper_results/evaluation/`. The tables below summarize the main findings.

**Entity-Level Performance (F1 Score):**

| Model | Precision | Recall | F1 |
|-------|-----------|--------|-----|
| CRF | 88.3% | 84.7% | 86.5% |
| BiLSTM+FastText | 80.7% | 77.4% | 79.0% |
| BERTimbau+Linear | 82.5% | 95.3% | 88.5% |
| BERTimbau+CRF | 88.4% | 93.5% | 90.9% |
| DeBERTa+Linear | 87.1% | 95.6% | 91.2% |
| **DeBERTa+CRF** â­ | **91.1%** | **95.0%** | **93.0%** |
| XLM-R+Linear | 76.3% | 93.6% | 84.1% |
| XLM-R+CRF | 88.9% | 94.7% | 91.7% |

â­ **Best model available on Hugging Face**: [`Anonymous3445/DeBERTa-CRF-VotIE`](https://huggingface.co/Anonymous3445/DeBERTa-CRF-VotIE)

**Event-Level Performance:**

| Model | Event Detection F1 | Subject F1 | Counting F1 | Participant-Vote F1 | Complete Event F1 |
|-------|-------------------|------------|-------------|---------------------|-------------------|
| CRF | 97.1% | 78.2% | 99.4% | 90.8% | 69.8% |
| BiLSTM+FastText | 96.2% | 70.1% | 96.2% | 86.0% | 58.4% |
| BERTimbau+Linear | 97.3% | 89.7% | 98.5% | 96.8% | 81.7% |
| BERTimbau+CRF | 96.9% | 89.2% | 99.2% | 97.9% | 85.6% |
| DeBERTa+Linear | 97.0% | 91.0% | 99.2% | 97.9% | 84.4% |
| **DeBERTa+CRF** â­ | **97.1%** | **92.2%** | **99.6%** | **97.8%** | **88.3%** |
| XLM-R+Linear | 97.3% | 79.7% | 97.1% | 93.3% | 70.0% |
| XLM-R+CRF | 97.2% | 90.2% | 99.6% | 98.1% | 86.5% |

â­ **Best overall performance** from DeBERTa+CRF, particularly on complete event extraction (88.3%)

### Gemini 2.0 Flash - Direct Event Extraction

In addition to sequence labeling approaches, we evaluated **Gemini 2.0 Flash** for direct event extraction using few-shot prompting. Unlike the other models that first extract BIO-tagged entities and then construct events, Gemini directly predicts complete voting events in structured JSON format.

**Approach:**
- **Model**: `gemini-2.0-flash-exp`
- **Method**: Few-shot learning with 10 curated examples
- **Task**: Direct extraction of voting events (subject, participants, counting, voting expressions)
- **Prompt**: Structured JSON output with predefined schema (see `src/gemini/shared/prompts.py`)

**Prompt Template:**
```
You are an expert at extracting structured information from Portuguese municipal meeting minutes.

Your task is to identify and extract voting events from city council meeting texts.

Output Format: Return ONLY a valid JSON object with this exact structure:
{
  "has_voting_event": boolean,
  "event": {
    "subject": string or null,
    "participants": [{"text": string, "position": "Favor"|"Against"|"Abstention"|"Absent"}],
    "counting": [{"text": string, "type": "Majority"|"Unanimity"}],
    "voting_expressions": [string]
  }
}
```

**Few-Shot Examples:**
10 examples from `src/gemini/shared/few_shot_examples.json` covering diverse voting scenarios across different municipalities (M01-M06).

**Usage:**
```bash
# Run Gemini extraction
cd src/gemini
python run_gemini_events.py --test  # Test mode (10 documents)
python run_gemini_events.py         # Full evaluation (433 documents)
```

**Note**: Gemini was evaluated solely for event-level extraction and was not compared at the entity-level (span extraction) with other models. See `src/gemini/README.md` for detailed documentation.

### Reproducing Paper Results

Published predictions and evaluation metrics are provided in `paper_results/`:
```bash
# View paper predictions
ls paper_results/predictions/

# View paper evaluation metrics
ls paper_results/evaluation/

# Example: Check DeBERTa-CRF results
cat paper_results/evaluation/deberta_crf.json
```

---

## Repository Structure

```
votie/
â”œâ”€â”€ configs/                      # Model configuration files (tracked in Git)
â”‚   â”œâ”€â”€ crf.yaml
â”‚   â”œâ”€â”€ bilstm_fasttext.yaml
â”‚   â”œâ”€â”€ bert_linear.yaml
â”‚   â”œâ”€â”€ bert_crf.yaml
â”‚   â”œâ”€â”€ deberta_linear.yaml
â”‚   â”œâ”€â”€ deberta_crf.yaml
â”‚   â”œâ”€â”€ xlmr_linear.yaml
â”‚   â””â”€â”€ xlmr_crf.yaml
â”‚
â”œâ”€â”€ data/                        # Dataset files
â”‚   â”œâ”€â”€ data_examples.json       # 30 sample examples (AVAILABLE)
â”‚   â”œâ”€â”€ data_statistics.json     # Sample data statistics (AVAILABLE)
â”‚   â”œâ”€â”€ votie_bio/               # BIO-tagged NER data (NOT YET AVAILABLE)
â”‚   â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”‚   â”œâ”€â”€ dev.jsonl
â”‚   â”‚   â”œâ”€â”€ test.jsonl
â”‚   â”‚   â””â”€â”€ statistics.json
â”‚   â””â”€â”€ votie_events/            # Event-level annotations (NOT YET AVAILABLE)
â”‚       â”œâ”€â”€ train.json
â”‚       â”œâ”€â”€ dev.json
â”‚       â”œâ”€â”€ test.json
â”‚       â””â”€â”€ statistics.json
â”‚
â”œâ”€â”€ paper_results/                # Published results from 
â”‚   â”œâ”€â”€ predictions/             # Model predictions on 
â”‚   â”‚   â”œâ”€â”€ crf.jsonl
â”‚   â”‚   â”œâ”€â”€ deberta_crf.jsonl
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ evaluation/              # Evaluation metrics
â”‚       â”œâ”€â”€ crf.json
â”‚       â”œâ”€â”€ deberta_crf.json
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ scripts/                     # Executable scripts
â”‚   â”œâ”€â”€ evaluate_sample_data.py  # Evaluate HF model on sample data (RECOMMENDED FOR REVIEWERS)
â”‚   â”œâ”€â”€ quick_start.py           # Quick demo using HuggingFace model
â”‚   â”œâ”€â”€ train.py                 # Unified training script
â”‚   â”œâ”€â”€ predict.py               # Prediction with trained models
â”‚   â”œâ”€â”€ evaluate.py              # Comprehensive evaluation
â”‚   â”œâ”€â”€ run_pipeline.py          # End-to-end pipeline
â”‚   â””â”€â”€ run_all_experiments.sh   # Run all experiments sequentially
â”‚
â”œâ”€â”€ src/                         # Source code
â”‚   â”œâ”€â”€ models/                  # Model implementations
â”‚   â”‚   â”œâ”€â”€ base.py              # Base model interface
â”‚   â”‚   â”œâ”€â”€ crf.py               # Traditional CRF
â”‚   â”‚   â”œâ”€â”€ bilstm_crf.py        # BiLSTM+FastText+CRF
â”‚   â”‚   â”œâ”€â”€ bertimbau_models.py  # BERTimbau models
â”‚   â”‚   â”œâ”€â”€ deberta_models.py    # DeBERTa models
â”‚   â”‚   â””â”€â”€ xlmr_models.py       # XLM-RoBERTa models
â”‚   â”œâ”€â”€ data/                   # Data loading and processing
â”‚   â”‚   â””â”€â”€ dataset.py          # Dataset utilities, windowing
â”‚   â”œâ”€â”€ evaluation/             # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ entity_metrics.py   # Entity-level (seqeval)
â”‚   â”‚   â””â”€â”€ event_metrics.py    # Event-level metrics
â”‚   â”œâ”€â”€ utils/                  # Utility functions
â”‚   â”‚   â””â”€â”€ event_constructor.py  # Event reconstruction
â”‚   â”œâ”€â”€ gemini/                  # Gemini API integration
â”‚   â”‚   â”œâ”€â”€ run_gemini_events.py # Direct event extraction script
â”‚   â”‚   â”œâ”€â”€ shared/
â”‚   â”‚   â”‚   â”œâ”€â”€ model.py         # GeminiEventExtractor
â”‚   â”‚   â”‚   â”œâ”€â”€ prompts.py       # Few-shot prompts
â”‚   â”‚   â”‚   â””â”€â”€ few_shot_examples.json  # Curated examples
â”‚   â”‚   â””â”€â”€ README.md            # Gemini module documentation
â”‚   â”œâ”€â”€ trainer.py              # Transformer training loop
â”‚   â””â”€â”€ bilstm_trainer.py       # BiLSTM training loop
â”‚
â”œâ”€â”€ models/                      # Local trained models (NOT tracked - too large)
â”œâ”€â”€ predictions/                 # Local predictions (NOT tracked)
â”œâ”€â”€ evaluation/                  # Local evaluations (NOT tracked)
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ LICENSE                      # License file (CC-BY-NC-ND 4.0)
â””â”€â”€ .gitignore                   # Git ignore rules
```

**Note on Directory Structure:**
- **Tracked in Git**: Source code, configs, sample data (`data_examples.json`, `data_statistics.json`), and `paper_results/` (for reproducibility)
- **NOT tracked**: `models/`, `predictions/`, `evaluation/` (generated locally, too large for Git)
- **NOT yet available**: Full dataset files in `data/votie_bio/` and `data/votie_events/` directories
- **Sample data available**: 30 annotated examples in `data/data_examples.json` - test these in the [ðŸŽ¯ Interactive Demo](https://huggingface.co/spaces/Anonymous3445/VotIE-demo)

---

## Reproducibility

### Fixed Random Seeds

All experiments use fixed random seeds for reproducibility:
- PyTorch seed: 42
- NumPy seed: 42
- Python random seed: 42
- CUDA deterministic mode: enabled

### Hardware Specifications

| Component | Specification |
|-----------|---------------|
| GPU | NVIDIA  L40 GPU |
| VRAM | 48GB |
| CUDA Version | 11.7+ |



### Reproducing Published Results

> **âš ï¸ Note**: The training pipelines below require the full VotIE dataset, which is not yet available in this repository. For now, you can:
> - Test the **pre-trained model** from [HuggingFace](https://huggingface.co/Anonymous3445/DeBERTa-CRF-VotIE)
> - Explore the **30 sample examples** in [`data/data_examples.json`](data/data_examples.json)
> - Try the **[ðŸŽ¯ Interactive Demo](https://huggingface.co/spaces/Anonymous3445/VotIE-demo)** to see the model in action

**Option 1: Using the pipeline script for each model** (requires full dataset)
```bash
# Run complete pipeline for each configuration
python scripts/run_pipeline.py --config configs/crf.yaml --experiment-name paper_reproduction
python scripts/run_pipeline.py --config configs/bilstm_fasttext.yaml --experiment-name paper_reproduction
python scripts/run_pipeline.py --config configs/bert_crf.yaml --experiment-name paper_reproduction
python scripts/run_pipeline.py --config configs/deberta_crf.yaml --experiment-name paper_reproduction
python scripts/run_pipeline.py --config configs/xlmr_crf.yaml --experiment-name paper_reproduction
```

**Option 2: Using the batch script** (requires full dataset)
```bash
# Run all experiments at once
bash scripts/run_all_experiments.sh
```

**Option 3: Manual step-by-step** (requires full dataset)
```bash
# 1. Train all baselines
for config in configs/*.yaml; do
  python scripts/train.py --config $config --experiment-name paper_reproduction
done

# 2. Generate predictions on test set
for model_dir in models/*/paper_reproduction/best_model; do
  model_name=$(basename $(dirname $(dirname $model_dir)))
  python scripts/predict.py $model_dir data/votie_bio/test.jsonl predictions/${model_name}_predictions.jsonl
done

# 3. Evaluate all predictions
for pred_file in predictions/*_predictions.jsonl; do
  python scripts/evaluate.py $pred_file evaluation/$(basename $pred_file .jsonl)_results.json
done
```

---

## License

This project is licensed under **CC-BY-ND 4.0 (Creative Commons Attributionâ€“NoDerivatives 4.0 International)**.

You are free to:

- **Share:** Copy and redistribute the material in any medium or format  

Under the following terms:

- **Attribution:** You must give appropriate credit.  
- **No Derivatives:** If you remix, transform, or build upon the material, you may not distribute the modified version.

For details, see the `LICENSE` file.

### Dataset License

The **Council Metadata Corpus** is derived from public municipal meeting minutes and is provided strictly for **research purposes only**.  
Original documents remain the copyright of their respective municipal governments.


### Dataset License

The VotIE corpus is derived from public municipal meeting minutes and is provided strictly for **research purposes only**. Original documents remain the copyright of their respective municipal governments.

By using this dataset, you agree to:
- Use it only for non-commercial research
- Cite the original paper
- Not redistribute without permission

---

## Acknowledgments

- The six Portuguese municipalities (M01â€“M06) for providing access to meeting minutes
- The [seqeval](https://github.com/chakki-works/seqeval) project for sequence evaluation metrics
- [Hugging Face](https://huggingface.co/) for the Transformers library and model hosting
- [Neuralmind](https://neuralmind.ai/) for BERTimbau pre-trained models

---



**Last Updated**: October 2025
